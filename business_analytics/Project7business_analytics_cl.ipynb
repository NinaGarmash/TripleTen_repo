{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b23ae0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Project description\n",
    "You've been offered an internship in the analytical department at Yandex.Afisha. Your first task is to help optimize marketing expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e6cdf",
   "metadata": {},
   "source": [
    "## Description of the data\n",
    "The **visits** table (server logs with data on website visits):\n",
    "* Uid — user's unique identifier\n",
    "* Device — user's device\n",
    "* Start Ts — session start date and time\n",
    "* End Ts — session end date and time\n",
    "* Source Id — identifier of the ad source the user came from.\n",
    "\n",
    "All dates in this table are in YYYY-MM-DD format.\n",
    "\n",
    "The **orders** table (data on orders):\n",
    "* Uid — unique identifier of the user making an order\n",
    "* Buy Ts — order date and time\n",
    "* Revenue — Yandex.Afisha's revenue from the order\n",
    "\n",
    "The **costs** table (data on marketing expenses):\n",
    "* source_id — ad source identifier\n",
    "* dt — date\n",
    "* costs — expenses on this ad source on this day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c9b0f",
   "metadata": {},
   "source": [
    "## Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0d92fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32ce621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sidetable in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from sidetable) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from pandas>=1.0->sidetable) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from pandas>=1.0->sidetable) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from pandas>=1.0->sidetable) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vladimir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->sidetable) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sidetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41da55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sidetable\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dfd18",
   "metadata": {},
   "source": [
    "# Step 1. Download the data and prepare it for analysis\n",
    "Store the data on visits, orders, and expenses in variables. Optimize the data for analysis. Make sure each column contains the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bb5733",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/visits_log_us.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14496\\3581059876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mvisits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visits_log_us.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Device'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Start Ts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'End Ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'visits_log_us.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14496\\3581059876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mvisits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visits_log_us.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Device'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Start Ts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'End Ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mvisits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/datasets/visits_log_us.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Device'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Start Ts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'End Ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0morders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'orders_log_us.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Buy Ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/visits_log_us.csv'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    visits = pd.read_csv('visits_log_us.csv', dtype = {'Device':'category'}, parse_dates=['Start Ts', 'End Ts'])\n",
    "except:\n",
    "    visits = pd.read_csv('/datasets/visits_log_us.csv', dtype = {'Device':'category'}, parse_dates=['Start Ts', 'End Ts'])\n",
    "try:\n",
    "    orders = pd.read_csv('orders_log_us.csv', parse_dates=['Buy Ts'])\n",
    "except:\n",
    "    orders = pd.read_csv('/datasets/orders_log_us.csv',parse_dates=['Buy Ts'])  \n",
    "try:\n",
    "    costs = pd.read_csv('costs_us.csv',parse_dates=['dt'])\n",
    "except:\n",
    "    costs = pd.read_csv('/datasets/costs_us.csv',parse_dates=['dt'])  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc943dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.stb.missing(style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91376e",
   "metadata": {},
   "source": [
    "The table contains 359400 rows 5 columns of non-missing values. All datatypes are correct, no duplicates found. Let's change column names to lowercase and replace spaces with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.columns=visits.columns.str.lower()\n",
    "visits.columns=visits.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c249873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3e9e5",
   "metadata": {},
   "source": [
    "Now **visits** table is ready for the analysis. Let's check **orders** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94371428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924bee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32643ec2-e657-4bb9-939b-1e9daec6e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.stb.missing(style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dfd74",
   "metadata": {},
   "source": [
    "Everything is ok except column names. Let's change them to lowercase and replace spaces with underscores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69631ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.columns=orders.columns.str.lower()\n",
    "orders.columns=orders.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88febe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.query('revenue == 0').revenue.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea20237",
   "metadata": {},
   "source": [
    "Now **orders** table is ready too. Let's check **costs** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdabd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c8e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b9fb6",
   "metadata": {},
   "source": [
    "The smallest dataset contains 2542 rows 3 columns of non-missing values, no duplicates. All datatypes and column names don't need changing. *costs* table is ready for the analysis.\n",
    "\n",
    "The 3 datasets are interrelated: user_ids in visits are the same as user_ids in orders, source_ids in visits are the same  as source_ids in costs, they serve as foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['start_ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bfe3a",
   "metadata": {},
   "source": [
    "Let's add columns with date, week, month values for visits and orders, they will be necessary for cohort analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66186d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['date']=visits['start_ts'].dt.to_period('D').dt.to_timestamp()\n",
    "visits['week'] = visits['start_ts'].dt.to_period('W').dt.to_timestamp()\n",
    "visits['month'] = visits['start_ts'].dt.to_period('M').dt.to_timestamp()\n",
    "orders['date']=orders['buy_ts'].dt.to_period('D').dt.to_timestamp()\n",
    "orders['week'] = orders['buy_ts'].dt.to_period('W').dt.to_timestamp()\n",
    "orders['month'] = orders['buy_ts'].dt.to_period('M').dt.to_timestamp()\n",
    "costs['month'] = costs['dt'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935ed1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535d6fd-4fa8-44c6-8fa6-7d4e1fad898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatype of new columns\n",
    "visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db92dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56f290-7d92-483a-82f5-f5554f738dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bca5f-2640-43cb-b34b-e6b88797b595",
   "metadata": {},
   "source": [
    "**Conclusion**. Three dataframes were downloaded, no missing values and duplicates found, all datatypes are correct. Column names were changed to lowercase, spaces were replaced with underscores for consistency. Columns containing dates, weeks, months for were added, they are in datetime64[ns] format. Now we have **visits**, **orders** and **costs** datasets ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53090495",
   "metadata": {},
   "source": [
    "# Step 2. Make reports and calculate metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15cd6f",
   "metadata": {},
   "source": [
    "## Product\n",
    "### How many people use it every day, week, and month?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a3c28",
   "metadata": {},
   "source": [
    "Let's find period of time the data is collected over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.start_ts.describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c723b",
   "metadata": {},
   "source": [
    "So the data is stored over the year from June 2017 till May 2018. If we group data by date, week, month and count unique users per period, it'll give us **DAU, WAU** and **MAU** metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec8dcd-1af6-49b0-8303-8b8f31bfdbe6",
   "metadata": {},
   "source": [
    "#### DAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e30520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dau=visits.groupby(visits['date']).uid.nunique().reset_index()\n",
    "dau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab95e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dau.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca35f9c",
   "metadata": {},
   "source": [
    "So DAU varies from 1 to 3319 users per day, with mean value around 908(median is a little greater - 921). Let's plot line graph for DAU over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a94e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(dau,x=\"date\", y=\"uid\",title='DAU')\n",
    "#adding reference line with average DAU over time\n",
    "fig.add_hline(y=dau['uid'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average DAU\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f117f7",
   "metadata": {},
   "source": [
    "For nearly half a year(October - March) DAU is above average with the peak value  on Nov.24 (BLACK FRIDAY?), before the winter holidays(Cristmass, New Year). There's also a peak on May 31(end of the scool year?), a rise on March 25 then drop down to min values on March 31(maybe after spring holidays or some technical issues with the app?). By  July 17 the line suddenly rises and falls -  maybe there was some add compain or another factor influensed it.  We can also see small regular rises and falls during the week. Let's check if number  of users varys with day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['dow'] = visits['date'].dt.dayofweek\n",
    "visits.dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6003b-5510-41f2-a0ff-328c897dca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_visits = visits.groupby(['dow'])['uid'].count().reset_index()\n",
    "dow_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e8c5d-8837-45f3-8e22-2f2171b8c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_visits.plot(kind='bar', x='dow', y='uid', color='blue')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('number of visits')\n",
    "plt.title('Number of visits:  Day of Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be4187-5e3d-418d-9791-cc8c991f0ca0",
   "metadata": {},
   "source": [
    "So, people open the app most often on Wednesday, then on Sunday, while min number of visits is on Saturday and Friday. These fluctuations form certain week cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fcb50-34eb-4bcf-8296-345234376759",
   "metadata": {},
   "source": [
    "#### WAU.\n",
    "Now let's turn to weekly visits and WAU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c787c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wau=visits.groupby(visits['week']).uid.nunique().reset_index()\n",
    "wau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79535f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wau.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5f520",
   "metadata": {},
   "source": [
    "WAU varies from 2021 to 10586 users per week, the mean is 5716 (the median is 5740 - not much greater). Let's plot line graph for WAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a0daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(layout=go.Layout(\n",
    "        title=go.layout.Title(text=\"WAU\")))\n",
    "fig.add_trace(go.Scatter(x=wau['week'], \n",
    "                        y=wau['uid'],\n",
    "                    mode='lines+markers',\n",
    "                    name='lines+markers',line = dict(color='purple', width=5)))\n",
    "#adding reference line with average DAU over time\n",
    "fig.add_hline(y=wau['uid'].mean(),line_dash=\"dash\", line_color=\"red\", annotation_text=\"average WAU\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded1abe",
   "metadata": {},
   "source": [
    "Again we see max WAU in the end of november, minor peaks on October 2, at the end of January 29, March 19. min values are in July-August weeks. As a whole WAU is below average from the end of March till the middle of September."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32af45-bb4a-437c-ae6d-998417b46b0c",
   "metadata": {},
   "source": [
    "#### MAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mau=visits.groupby(visits['month']).uid.nunique().reset_index()\n",
    "mau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8741a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mau.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff6896",
   "metadata": {},
   "source": [
    "MAU varies from 116331 to 32797 users per month, the mean is 23228. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot=sns.barplot(data=mau, x=\"month\", y=\"uid\")\n",
    "plot.set(title=\"MAU\")\n",
    "plot.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ffef5",
   "metadata": {},
   "source": [
    "Again we see MAU above average in October - March with max values in November and December. Since Aprile MAW is below average, in summer people buy less, min MAU value is in August."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b8e6f",
   "metadata": {},
   "source": [
    "### How many sessions are there per day? per user?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00373d",
   "metadata": {},
   "source": [
    "Since one user might have more than one session, let's calculate **number of sessions per day** by counting all users (not just unique) per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7be654",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_day=visits.groupby(['date'])['uid'].count().reset_index()\n",
    "sessions_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a751e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sessions_day.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07c5c3",
   "metadata": {},
   "source": [
    "Number of sessions per day varies from 1 to 4042, the mean is 987 (the median is 1003 -  there's some right skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(sessions_day,x=\"date\", y=\"uid\",title='Number of Sessions per Day')\n",
    "#adding reference line with average DAU over time\n",
    "fig.add_hline(y=sessions_day['uid'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average sessions_day\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00ba46",
   "metadata": {},
   "source": [
    "Let's find average **number of sessions per user** by dividing total number of sessions per day by DAU and explore the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cad5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_dynamics=pd.merge(dau, sessions_day, on='date', how='inner')\n",
    "day_dynamics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dynamics.columns =['date', 'dau', 'sessions_day']\n",
    "day_dynamics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cca27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_dynamics['sessions_user'] = day_dynamics['sessions_day']/ day_dynamics['dau']\n",
    "day_dynamics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02c553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_dynamics.sessions_user.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a3257",
   "metadata": {},
   "source": [
    "So, number of sessions per user varies from 1 to 1.22 with the mean 1.08. Let's plot line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e739caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot sessions per user\n",
    "fig = px.line(day_dynamics,\n",
    "              x=\"date\", y=\"sessions_user\",title='Sessions per user')\n",
    "fig.add_hline(y=day_dynamics['sessions_user'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average sessions_user\",\n",
    "             annotation_position=\"top left\")         \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348c213",
   "metadata": {},
   "source": [
    "**Number of sessions per user** is very stable, normally distributed (with mean = median = 1.08 and std  only 0.02)\n",
    "There are regular weekly cycles of ups and downs. Max value is again on Nov 24(Black Friday), smaller peaks on Feb 1, Mar 26, May 31, Jun 7, Jul 17, min on Mar 31."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e5bc6-fdde-419f-b338-ef0b6a7e1fa9",
   "metadata": {},
   "source": [
    "Although number of sessions per day is greater than DAU, the overall picture of distribution  of these metrics with  peaks and week cycles is pretty similar. If we plot them on the same graph it will be more evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e709437",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=day_dynamics['date'], \n",
    "                        y=day_dynamics['dau'],\n",
    "                    mode='lines',\n",
    "                    name='DAU',line = dict(color='yellow', width=2 )))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=day_dynamics['date'], \n",
    "                        y=day_dynamics['sessions_day'], \n",
    "                       \n",
    "                    mode='lines',\n",
    "                    name='sessions',line = dict(color='black', width=2 )))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    plot_bgcolor=\"grey\",\n",
    "    margin=dict(t=10,l=10,b=10,r=10)\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1476be6-11db-4f49-b7be-c10cfd44020e",
   "metadata": {},
   "source": [
    "So, the line for number of sessions per day almost repeats the line for DAU a little higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb15781",
   "metadata": {},
   "source": [
    "### What is the length of each session?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cdac8",
   "metadata": {},
   "source": [
    "We have start_ts and end_ts for each session, so **session length** will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94215637",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['session_length']= visits['end_ts']-visits['start_ts']\n",
    "visits.session_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.session_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b7511",
   "metadata": {},
   "source": [
    "There's min session_length value -1 day + 23:14:00, which is absolutely impossible. Let's find the row in visits table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c31d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits.query('session_length ==\"-1 days +23:14:00\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65474a",
   "metadata": {},
   "source": [
    "Here start_ts is greater than end_ts and give negative session_length - that's an error! What if there is more than 1 such case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c89ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits.query('start_ts > end_ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b462f",
   "metadata": {},
   "source": [
    "There're only 2 cases, both on the same day almost the same time(at night!), so maybe there were problems with internet connection or some other technical issues. We do not now if start_ts and end_ts just switched places or were recorded incorrectly, so we can't restore them. Let's just exclude the rows from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9afa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits=visits.query('start_ts <= end_ts')\n",
    "visits.session_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e27c62",
   "metadata": {},
   "source": [
    "Mean session_length is 10 min 43sec, while median is much lower - only 5 min, so the distribution is right skewed. Let's plot a histogtam. First datatypes should be changed and values turned to minutes as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ea2ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits.session_length.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ad829",
   "metadata": {},
   "source": [
    "Converting timedelta to numeric representation in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d887bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['session_length_minutes'] = visits['session_length'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61a9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visits['session_length_minutes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf61ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the histogram\n",
    "visits['session_length_minutes'].hist()\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Session Length (Minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Session Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daedc19",
   "metadata": {},
   "source": [
    "There're outliers with length up to 711 min, but 75% of data is below 14 min. Let's 'zoom' the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e0280",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "visits['session_length_minutes'].hist(bins=100, range = (0,30))\n",
    "plt.xlabel('Session Length (Minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Session Length')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72f7d6-6cb6-4ff3-8bc2-6bffddcc074c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many sessions last less than 15 sec\n",
    "visits.query('session_length_minutes < 0.25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5264c-226e-42b3-9e9f-a3916d77b7fd",
   "metadata": {},
   "source": [
    "35794 sessions last less than 0.25 minutes(15 sec!). What is it in %?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a80e5-48f6-40f2-a0e8-ce65c7498038",
   "metadata": {},
   "outputs": [],
   "source": [
    "35794/ visits['uid'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb94c7f",
   "metadata": {},
   "source": [
    "The distribution is left skewed, 50% of values are below 5 min. 10% of sessions last less than 15 sec, that can be a signal of some issues with the app(downloading, registration and so on). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587d8ee",
   "metadata": {},
   "source": [
    "### What's the user retention rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e37a74",
   "metadata": {},
   "source": [
    "**Retention** shows us how many users (in % out of registered) had sessions on a certain day/week/month after first visit. Let's find the first session as min visit date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d38740",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ses=visits.groupby(['uid'])['date'].min().reset_index()\n",
    "first_ses.columns = ['uid', 'first_session']\n",
    "first_ses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a500c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now merging to the original dataset\n",
    "visits=visits.merge(first_ses, how='left',on=['uid'])\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f94b4",
   "metadata": {},
   "source": [
    "For further cohort analysis we will define two parameters:\n",
    "\n",
    "**cohort**: monthly cohort when the user had first session (what month the user had his first session determines which cohort he belongs to),  \n",
    "**age**: the difference between any current session and first_session calculated in months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['cohort'] = visits['first_session'].dt.to_period('M').dt.to_timestamp()\n",
    "visits['age'] = ((pd.to_datetime(visits['date']) - pd.to_datetime(visits['first_session'])) / np.timedelta64(1,'M'))\\\n",
    "                                                                        .round().astype('int')\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3af9c",
   "metadata": {},
   "source": [
    "Now cohorts are ready, age is calculated. Lets see how many users were active from certain cohorts on a certain time after registration with pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = visits.pivot_table(index='cohort',\n",
    "                  columns='age',\n",
    "                  values='uid',\n",
    "                  aggfunc='nunique').fillna(0)\n",
    "cohorts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd1682",
   "metadata": {},
   "source": [
    "To calculate **retention** we find a % of those who  are still active from those who registered, that is  we divide columns for all ages by column when age=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retention=cohorts.iloc[:,0:].div(cohorts[0], axis=0)\n",
    "retention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7305eb8",
   "metadata": {},
   "source": [
    "To compare retention rate for different cohorts at different ages heatmap will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390123ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the first line just formats cohort into a str for a nicer output\n",
    "retention.index=retention.index.astype(str)\n",
    "\n",
    "sns.heatmap(retention, annot=True, fmt='.1%', linewidths=1, linecolor='grey', vmax=0.1, cbar_kws= {'orientation': 'horizontal'} \n",
    "            ).set(title = 'Retention Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2accf",
   "metadata": {},
   "source": [
    "The heatmap clearly shows the retention rate(RR) dynamics. All cohorts' RR0(retention rate at the age of 0 months) > RR1 > RR2. But futher there are 2 variants.\n",
    "*  The first cohort(starting in June) in a month has 7.5 of its users still active, then only 5.4%. At the age of 3 months retention rate rises to 6.2% and remains relatively high for 3 months, than gradually falls to 0.7% at the 12th month. The second(July) cohort has smaller retention rate in a month - only 5.7%(we already know that in august all users show min activity)> then it was smaller for 2 months and again rose to 5.7 in the age of 5 months( which was November the great!). The first two cohorts had the same pattern:  RR0> RR1> RR2 < RR3 < RR4 > RR5 > RR6..., that is after initial decline of retention rate we see a certain rise, then fall again. \n",
    "*  All  other cohorts have no rise, their RR is only descending. \n",
    "\n",
    "Max retention at 1 month age  - 7.8% - was at September cohort.\n",
    "February, March, April cohorts have RR less than 5 % from the very beginning.\n",
    "At the end of the period observed(May) max retention - 4 % - was demonstrated by June cohort(its 12th month) and April cohort(age 1 month - no wonder)! June data are incomplete and won't be analized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f82f0-92e4-42ea-9fd7-0fd8033edddb",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235ed9b-cecc-44bc-9cd1-1b52c32e32fa",
   "metadata": {},
   "source": [
    "Metrics about Product were calculated, they show you how users interact with the app. Overall average\n",
    "**DAU**  is 908 unique users per day, average **WAU** is 5716 unique users per week, average **MAU** is 23228 unique users per month. Their dynamics over the year has much in common.\n",
    "For nearly half a year(October - March) DAU, WAU and MAU are above average. with the peak value on Nov.24 (BLACK FRIDAY?), before the winter holidays(Cristmass, New Year). Since Aprile DAU, WAU, MAU  are below average,  min values are in august(people are on vacation). There's a sudden drop down around Mar 31 for all metrics. We can also see small regular rises and falls during the week as weekly cycle: people open the app most often on Wednesday, then on Sunday, while min number of visits is on Saturday and Friday. \n",
    "\n",
    "**Number of sessions per day** on average is 987 - it is greater than DAU, but the distribution over the year, peaks and falls, week cycles are pretty similar(line graphs almost coincide).\n",
    "\n",
    "**Number of sessions per user** varies from 1 to 1.22 with the mean 1.08. Unlike other metrics,it is normally distributed, but there are also regular weekly ups and downs, max value is again on Nov 24(Black Friday), min on Mar 31 and in August.\n",
    "\n",
    "Mean value of **session length** is 10 min 43sec and some users hang on the site up to 700 min, but 50% of sessions last less than 5 min, **10% of sessions last less than 15 sec**, that can be a signal of some issues with the app.\n",
    "\n",
    "**Cohort analysis**  was made for calculating **retention rate**(RR) as a % of those users who are still active at a certain period of time. Heatmap for RR  for all cohorts at all ages was ploted and RR dynamics was determined.\n",
    "All cohorts' RR0(retention rate at the age of 0 months) > RR1 > RR2. But then there are 2 scenarios:\n",
    "-  after initial fall RR can rise again and after 2-3 months begin to fall again(June and July cohorts)\n",
    "-  no rise, RR is falling every month(all other cohorts).\n",
    "RR in the first 2 cohorts may rise because in october the general rise in user's activity begins(as we already know about DAU/WAU/MAU and other metrics' dynamics over the year).\n",
    "-  At the age of 1 month  max retention was 7.8% (September cohort). April, March cohorts activity is the least - RR1  around 4%.\n",
    "- At the end of the period observed(May) max retention - 4 % - was demonstrated by June cohort(its 12th month) and April cohort(age 1 month - no wonder)! June data are incomplete and won't be analized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c7906",
   "metadata": {},
   "source": [
    "## Sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3764fb",
   "metadata": {},
   "source": [
    "### Conversion \n",
    "One of the metrics describing sales is Conversion. **Conversion rate** is % of users who made an order(were converted to customers),  **conversion  time** depicts how long does it take from first visit to the purchase in days. So for each user we find the distance between the date of first visit and the date of first order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each user let's find the time of first order\n",
    "first_order=orders.groupby(['uid'])['date'].min().reset_index()\n",
    "first_order.columns = ['uid', 'first_order']\n",
    "first_order.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cbccd",
   "metadata": {},
   "source": [
    "Earlier we have already found first_session for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_ses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112315e6",
   "metadata": {},
   "source": [
    "Let's merge the info on first session and first order to one user_first table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_first = pd.merge(first_order, first_ses, on = 'uid', how='left')\n",
    "user_first.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fab70a",
   "metadata": {},
   "source": [
    "Let's calculate average conversion time and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380ab9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_first['conversion']=((user_first['first_order']-user_first['first_session'])/np.timedelta64(1,'D')).astype('int')\n",
    "user_first['conversion'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06dc332",
   "metadata": {},
   "source": [
    "Conversion varies from 0 to 363 days with the mean around 17, while the median is 0 and 75% of users make order within 2 days from the first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8708ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(user_first, x=\"conversion\",nbins=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31a1bb",
   "metadata": {},
   "source": [
    "There are many upper outliers, so let's zoom the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8be4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_first['conversion'].hist(bins=30, range = (0,30))\n",
    "plt.xlabel('conversion (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of conversion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095ac12-f207-4211-be74-7cbe74ab8f4a",
   "metadata": {},
   "source": [
    "So, on average it takes 0 days to convert for a user. But how many users converted at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b81e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(' The overall conversion is {:.1%}'.format(orders['uid'].nunique()/visits['uid'].nunique()))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a46f51",
   "metadata": {},
   "source": [
    "### How many orders do they make during a given period of time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c7ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging the info on first_order to the orders table \n",
    "orders=orders.merge(first_order, how='left',on=['uid'])\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fb575",
   "metadata": {},
   "source": [
    "Let's define cohort on the first order month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e85482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders['first_order_month']=orders['first_order'].dt.to_period('M').dt.to_timestamp()\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define cohort size\n",
    "cohort_sizes = orders.groupby('first_order_month')['uid']. nunique().reset_index()\n",
    "cohort_sizes.columns=['first_order_month','cohort_size']\n",
    "cohort_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87837d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculating number of purchases for cohort and month\n",
    "cohort=orders.groupby(['first_order_month','month'])['revenue'].count().reset_index()\n",
    "cohort.columns=['first_order_month','month','orders']\n",
    "cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge cohort with cohort size\n",
    "cohort=cohort.merge(cohort_sizes,on=['first_order_month'])\n",
    "cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find cohort's age in months\n",
    "cohort['age_month'] = ((cohort['month'] - cohort['first_order_month']) / np.timedelta64(1,'M')).round()\n",
    "cohort['orders_per_user']=cohort['orders']/cohort['cohort_size']\n",
    "cohort.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e99a6-13f1-4192-966c-8d476d33d67b",
   "metadata": {},
   "source": [
    "We see that every cohort has max number of orders per user(1.1-1.2) in its first month(age 0), then the value becomes many times smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040165e3",
   "metadata": {},
   "source": [
    "Let's make pivot table for cohorts' orders per user and compare  values for cohorts and months(age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83646142",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_piv=cohort.pivot_table(\n",
    "    index='first_order_month', \n",
    "    columns='age_month', \n",
    "    values='orders_per_user', \n",
    "    aggfunc='sum'\n",
    ").cumsum(axis=1)\n",
    "\n",
    "cohort_piv.round(2).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e35fc3-d9cd-4d61-b718-f1a36f2bc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_piv.index=cohort_piv.index.astype(str)\n",
    "sns.heatmap(cohort_piv, annot=True, fmt='.2f', linewidths=1, linecolor='grey', cbar_kws= {'orientation': 'horizontal'} \n",
    "            ).set(title ='Orders per User(cumulative)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe1343",
   "metadata": {},
   "source": [
    "At the age of 0 and 1 max orders per user is at November cohort - 1.18, 1.28 (no wonder, November is a the happiest month, December is also very active). After that(since age 2) the June cohort becomes the leader and has max cumulative orders per user, but November cohort still has very good results. April and May cohorts are the weakest - cum.orders/user 1.10, 1.09. There'sa also the 13th cohort(June) - someone has made orders on June 1, but it's data is incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2ae4b",
   "metadata": {},
   "source": [
    "### What is the average purchase size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total check for each user and plotting it\n",
    "avg_check=orders.groupby(['uid'])['revenue'].sum().reset_index()\n",
    "avg_check.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eccde",
   "metadata": {},
   "source": [
    "User's total check varies a lot - from 0 to 11810, mean revenue is 6.9, but the median is only 3, so the distribution is not normal, but right skewed. There's a big number of 0 revenues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.query('revenue == 0').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.query('revenue == 0').revenue.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cff0f-f347-4c4e-8d7a-67288c57ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share of 0 revenues\n",
    "51/orders['uid'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd913be-ba87-449c-8df1-9b7a47c37eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.query('revenue == 0')['uid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ee4d8-6c49-43af-8469-75456c293e33",
   "metadata": {},
   "source": [
    "Revenues occure on different dates, some users have it just once, but other users have 2-12 such sessions. Maybe the users want to order some tickets and then cansel it, or perchaps it was a child playing with the app.\n",
    "Share of 0 revenues is not big - 0.1 %, but 0 revenues from orders are impossible, there were no real orders. Let's exclude the rows with 0 revenues fron the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e128f7-ef08-4743-9ea3-34992492df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders=orders.query('revenue > 0')\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total check for each order and plotting it\n",
    "avg_check['revenue'].hist(bins=50)\n",
    "plt.xlabel('revenue')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of revenue')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476524f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_check['revenue'].hist(bins=50, range = (0,100))\n",
    "plt.xlabel('revenue')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24282b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohort analysis on averge check:\n",
    "\n",
    "avg_cohort=orders.groupby(['first_order_month','month'])['revenue'].mean().reset_index()\n",
    "avg_cohort['age_month'] = ((avg_cohort['month'] - avg_cohort['first_order_month']) / np.timedelta64(1,'M')).round()\n",
    "avg_cohort.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269e635-7794-4c7e-b6de-48f4ce9b091b",
   "metadata": {},
   "source": [
    "Now group data in a pivot table with mean revenue value for age(months):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b159cc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_cohort_piv=avg_cohort.pivot_table(\n",
    "    index='first_order_month', \n",
    "    columns='age_month', \n",
    "    values='revenue', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "avg_cohort_piv.round(2).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28380578-d424-4de8-b2f8-9afcd8be68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cohort_piv.index=avg_cohort_piv.index.astype(str)\n",
    "sns.heatmap(avg_cohort_piv, annot=True, fmt='.2f', linewidths=1, linecolor='grey', cbar_kws= {'orientation': 'horizontal'} \n",
    "            ).set(title ='Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a9316-71ed-4336-b68f-e4d0717abf76",
   "metadata": {},
   "source": [
    "At the age of 0 July cohort had max revenue(5.29). At the age of 1 month September cohort pulled ahead with  revenue 13.23, and it had absolute max revenue(62.57) at the age of 3 which was in December and was the leader till January.  We can see good results in December cohort, which became the first at its age 2(20.07 in February) and then in March, April, May. Min revenue brought January cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12ad8d",
   "metadata": {},
   "source": [
    "### How much money do they bring? (LTV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bf040",
   "metadata": {},
   "source": [
    "Ltv is the total amount of money the average customer brings to the company by making purchases. It's calculated as gross profit from a customer per period. For this project we can keep margin rate=1, so gp=revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the revenue per cohort in each month\n",
    "ltv_cohort=orders.groupby(['first_order_month','month'])['revenue'].sum().reset_index()\n",
    "ltv_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a241a3c-0518-4e14-851d-e6181dd5b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with the cohort size\n",
    "ltv_cohort=ltv_cohort.merge(cohort_sizes,on=['first_order_month'])\n",
    "ltv_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf004755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ltv_cohort['age']=((ltv_cohort['month'] - ltv_cohort['first_order_month']) / np.timedelta64(1,'M')).round()\n",
    "ltv_cohort['ltv']=ltv_cohort['revenue']/ltv_cohort['cohort_size']\n",
    "ltv_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LTV with cumsum in pivot table\n",
    "ltv_cohort_piv=ltv_cohort.pivot_table(\n",
    "    index='first_order_month', \n",
    "    columns='age', \n",
    "    values='ltv', \n",
    "    aggfunc='sum'\n",
    ").cumsum(axis=1)\n",
    "ltv_cohort_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f154f67-c2c8-4540-b2fa-643a1e269606",
   "metadata": {},
   "source": [
    "Let's visualize it with heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltv_cohort_piv.index=ltv_cohort_piv.index.astype(str)\n",
    "sns.heatmap(ltv_cohort_piv, annot=True, fmt='.2f', linewidths=1, linecolor='grey', cbar_kws= {'orientation': 'horizontal'} \n",
    "            ).set(title ='LTV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bf4f4-8788-450d-b906-d4ea8f8ad59b",
   "metadata": {},
   "source": [
    "Cumulative LTV naturally grows in all rows. Again September cohort is the leader - it has max LTV since age 1(Oktober) throgh most profitable Nov,Dec till May, reaching the value of 13.44. If we compare diagonal values, June cohort starts out - it has max of all cohorts LTV in Oct(age 4 - 7.62) and Nov, then it is the second after September cohort. The least profitable is February cohort with min LTV(4.59)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd988a-4a01-41cf-8fd0-9477b311c5fb",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Sales metrics were calculated.\n",
    "\n",
    "The overall **conversion**  is 16.0%, and 50% of users make order  and become customers on the same day as they are registered so the median of **conversion time** is 0.  75% of users make order within 2 days from the first session, but the mean conversion time is much higher (around 17), some users wait up to 363 days before ordering.\n",
    "\n",
    "Number of **orders per user** was calculated for month cohorts. Every cohort has max number of orders per user in its first month(age 0), then the value becomes many times smaller, but cumulative sum is gradually rising. At the age of 0 and 1 max cum orders per user is at November cohort - 1.18, 1.28 (no wonder, November is a the happiest month due to Black Friday, December is also very active). After that(since age 2) the June cohort becomes the leader and has max cumulative orders per user. April and May cohorts are the weakest - cum.orders/user 1.10, 1.09.\n",
    "\n",
    "User's total check for **revenue** is 6.9(the mean), but the median is only 3, so the distribution is  right skewed with outliers up to 11810.  0 revenues(0,1%) were excluded from cohort analysis.\n",
    "At the age of 0 July cohort had max revenue(5.29).  At the age of 1 month September cohort pulled ahead with  revenue 13.23, and it had absolute max revenue(62.57) at the age of 3 which was in December and was the leader till January.  We can see good results in December cohort, which became the first at its age 2(20.07 in February) and then in March, April, May. Min revenue brought January cohort.\n",
    "\n",
    "Cumulative **LTV** naturally grows in all rows. Again September cohort  brought to the company max money - it has max LTV since age 1(Oktober) throgh most profitable Nov,Dec till May, reaching the value of 13.44. The second is June cohort (cum LTV 11.88). The weakest is February cohort with min LTV(4.59).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da01ba2",
   "metadata": {},
   "source": [
    "##  Marketing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bab25",
   "metadata": {},
   "source": [
    "### How much money was spent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a27da6-78f3-47c4-81d3-30c2075eeccc",
   "metadata": {},
   "source": [
    "#### Cost: overall and per source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973dda1c",
   "metadata": {},
   "source": [
    "Here we deal with costs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6f13f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sorting cost for futher visualization\n",
    "costs=costs.sort_values(by=['dt','source_id'])\n",
    "costs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c11fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find total marketing\n",
    "print('Total marketing cost is {}'.format(costs['costs'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92f5e",
   "metadata": {},
   "source": [
    "So, **total amount spent on marketing**  by the company is $329131.62. Let's find its distribution per source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017af924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "marketing_source=costs.groupby(['source_id'])['costs'].sum()\n",
    "marketing_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fd8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of each source's cost over the total\n",
    "percentage = (marketing_source / marketing_source.sum() * 100).round(1)\n",
    "result = pd.DataFrame({'total_costs': marketing_source, 'percentage': percentage})\n",
    "result = result.sort_values('total_costs', ascending=False).reset_index()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the barplot for total costs per source\n",
    "result.plot(kind='bar', x='source_id', y='total_costs', color='blue')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Total Costs')\n",
    "plt.title('Marketing: Total Costs per Source')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bfe8d",
   "metadata": {},
   "source": [
    "**So**,  most of the money was spent on source 3(43% of costs, 141321.63 costs), the smallest amount was spent on sources 9,10 (less than 2%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dba455-b318-46a1-81f2-4780a07e8aa5",
   "metadata": {},
   "source": [
    "#### Costs over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeab179",
   "metadata": {},
   "source": [
    "Let's see how costs change  over time - each day, month, over the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e877d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(costs, x=\"dt\", y=\"costs\", color='source_id')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3f84f-24e8-4258-a4fc-fab5cbc82efe",
   "metadata": {},
   "source": [
    "Lines for all sources are shown on the graph. Source 3 has max values and max fluctuation range, soeces 9,10 have min values.The highest peak for all source is on Nov 24 - as expected, min costs were spent on Mar 31(end of quarter?) and in august(people on holidays). There're also some minor peaks for source1: Okt 6, Okt 27, Nov 3, Dec 11, Dec 28, Jan3, Mar 7, May 31, June 8. Close to them but not always matching are peaks for source 4: Dec 5,14, Jan 16,22, Feb 13, Mar 10,12, June 9, Jul 14. For some reason there's no special activity of source 4 around Nov 24 -  on the contrary, its costs are pretty smalll on Nov 24.\n",
    "Source 5  has max costs on Nov 24 oand just one more peak on May 31."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0ae1b-780f-449f-b98b-031e34d1e820",
   "metadata": {},
   "source": [
    "Let's unite all the sources and see the whole marketing costs distribution over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1556e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_date=costs.groupby(['dt'])['costs'].sum().reset_index()\n",
    "total_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f27732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.line(total_date,x=\"dt\", y=\"costs\",title='Costs per day')\n",
    "\n",
    "fig.add_hline(y=total_date['costs'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average costs\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135b45e",
   "metadata": {},
   "source": [
    "This graph is very similar to DAU/WAU/MAU/number of sessions per day lineplots with the same peak on Nov 24  and min costs spent in August and sudden fall on Mar 31."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645457f-1265-4d40-bbec-f194e8e41cf2",
   "metadata": {},
   "source": [
    "Let's compare with revenue distribution over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b5c8a-1d73-4154-a811-8aeeceae7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_date=orders.groupby(['date'])['revenue'].sum().reset_index()\n",
    "fig = px.line(revenue_date,x=\"date\", y=\"revenue\",title='Revenue per day')\n",
    "\n",
    "fig.add_hline(y=revenue_date['revenue'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average costs\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cb78c-f9a4-4d12-bce6-72d1b0daf634",
   "metadata": {},
   "source": [
    "There's less correlation than with DAU/...metrics. The highest peak is not on Nov 24, but on Dec 10 (related with max activity of source 5 supported by other sources? people buy more tickets before winter holidays?) and May 31. Revenue in January falls below average(users have already spent all the money) till Jan 26, Feb 1 and stay above average till the end of March. Min revenue on Mar 31 and in august correlates with min marketing costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c4f6d-08e2-46ad-adb2-efded7236366",
   "metadata": {},
   "source": [
    "Let's find monthly marketing costs and compare them with revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d413915",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_month=costs.groupby(['month'])['costs'].sum().reset_index()\n",
    "marketing_month.plot(kind='bar', x='month', y='costs', color='blue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Costs')\n",
    "plt.title('Marketing: Total Costs per Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389edb6-828b-4cd4-a565-bb462b048480",
   "metadata": {},
   "source": [
    "So, the largest costs are spent on marketing in December, November, October, the smallest costs are is August."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df05a6-afd1-431b-a2e3-a4bca0693757",
   "metadata": {},
   "source": [
    "In Oct, Nov and Dec marketing costs gradually grow, that correlates with growing revenues and max revenue in December. In January costs remain relatively high, though  revenue falls below average. Costs decrease from jan to March, while DAU/MAU doesn't change much and revenue per day grows towards March. Costs in June and July are the equal, but revenue and users activity rises significantly around July 17. Mayby marketing costs should support natural user's activity in February, March and July."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46333fc-ac16-4c57-a44d-f5596bdf8d93",
   "metadata": {},
   "source": [
    "There're also weekly fluctuations in costs, as in user activity metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db22556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs['dow'] = costs['dt'].dt.dayofweek\n",
    "total_dow=costs.groupby(['dow'])['costs'].sum()\n",
    "total_dow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29ada2",
   "metadata": {},
   "source": [
    "There's definetly a difference in costs depending on day of the week. Let's plot barplot(1 stays for Monday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f1af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_dow.plot(kind='bar', x='dow', y='costs', color='blue')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Total Costs')\n",
    "plt.title('Marketing: Total Costs per Day of Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef958d",
   "metadata": {},
   "source": [
    "So, the largest amount is spent on marketing on Wednesday, then Thursday and Sunday, min costs are on Saturday. We already now, that users open the app most often on Wednesday, then on Sunday, while min number of visits is on Saturday and Friday. Maybe the main target days should be  Wednesday and Sunday(not Thursday) to attract more customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3977bb",
   "metadata": {},
   "source": [
    "### How much did customer acquisition from each of the sources cost?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4c2f2-f522-4814-a245-9431781bac96",
   "metadata": {},
   "source": [
    "#### CAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ba0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers_month=orders.groupby(['first_order_month'])['uid'].nunique().reset_index()\n",
    "customers_month.columns=['month','customers']\n",
    "customers_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAC_month=marketing_month.merge(customers_month,how='left',on=['month'])\n",
    "CAC_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CAC_month['CAC']=CAC_month['costs']/CAC_month['customers']\n",
    "CAC_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(CAC_month, x=\"month\", y=\"CAC\", title='CAC')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4b9ff-219a-4c68-8a90-71ff35206f63",
   "metadata": {},
   "source": [
    "Max CAC is in August(10,8), when user's activity is the least,then it goes down till October(8.4), when users become more active. In November attracting a customer costs more than in December(users are active without special push) and March, but less than in January and April(users don't want to buy). Min CAC is in May(7,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a0e59-2325-4b4d-862f-e1f60957b027",
   "metadata": {},
   "source": [
    "#### CAC per source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4df70",
   "metadata": {},
   "source": [
    "Not all users have one source, let's define the first that they came from as the main source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89ad7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_source=visits.sort_values('date').groupby('uid').first()['source_id'].reset_index()\n",
    "first_source.columns=['uid','first_source']\n",
    "first_source.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2600c1",
   "metadata": {},
   "source": [
    "Merge this data back to orders becaue CAC calculations are based on buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d17212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders=orders.merge(first_source,on=['uid'],how='left')\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee647642",
   "metadata": {},
   "source": [
    "Let's see how CAC was changing for each source across time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22b6a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "marketing_month_source=costs.groupby(['month','source_id'])['costs'].sum().reset_index()\n",
    "marketing_month_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229820db",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_month_source=orders.groupby(['first_order_month','first_source'])['uid'].nunique().reset_index()\n",
    "customer_month_source.columns=['month','source_id','customers']\n",
    "customer_month_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af250343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging tables with data for cac\n",
    "CAC_month_source=marketing_month_source.merge(customer_month_source,how='left',on=['month','source_id'])\n",
    "CAC_month_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e5394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#actual cac calculations\n",
    "CAC_month_source['CAC']=CAC_month_source['costs']/CAC_month_source['customers']\n",
    "CAC_month_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting cac dynamics\n",
    "\n",
    "fig = px.line(CAC_month_source, x=\"month\", y='CAC',color='source_id',title='CAC')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bf586-fd76-4c73-8576-a168478f01c0",
   "metadata": {},
   "source": [
    "All seven sources are shown, their lines differ much. Sources 3,2,4 have max CAC in August,\n",
    "source 5 - in November, source 1 - in January, source 10 - in December, source 5 - in  April. All sources lines fall in May."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4efc5f",
   "metadata": {},
   "source": [
    "Lines for souces 3,2  are much higher than others. Let's calculate overall average CAC per source  and compare number of customers and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f8616-3b5c-4ecd-89f9-d167cedbc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_CAC=CAC_month_source.groupby(['source_id'])[['CAC','customers', 'costs']].agg({'customers': 'sum', 'costs': 'sum', 'CAC': 'mean'}).sort_values(by='CAC', ascending=False)\n",
    "source_CAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce99e3-18f4-4a7a-bf0a-448ec7149f72",
   "metadata": {},
   "source": [
    "So, sources 3 and 2 spend much more on attracting a customer than other sources and they bring many customers. But these sources are not so effective as sources 5, 4 ,1: source 4 brings almost as many customers as source 3, but each customers price is 2 times lower, source 1 brings the same number of customers as source 2 at 2 times lower cost. Maybe there's a possibility to reduce costs on less effective sources in favor of most effective sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719af83",
   "metadata": {},
   "source": [
    "### How worthwhile where the investments? (ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cb81d",
   "metadata": {},
   "source": [
    "#### ROI per cohort.\n",
    "\n",
    "In genereal, ROI=LTV/CAC\n",
    "\n",
    "We already have calculations on CAC per month and we have info on ltv in ltv_cohort. So let's merge and work it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAC_month_ROI=CAC_month[['month','CAC']]\n",
    "CAC_month_ROI.columns=['first_order_month','CAC']\n",
    "ROI=ltv_cohort.merge(CAC_month_ROI,on=['first_order_month'],how='left')\n",
    "ROI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977219cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI['ROI']=ROI['ltv']/ROI['CAC']\n",
    "roi_piv = ROI.pivot_table(\n",
    "    index='first_order_month', columns='age', values='ROI', aggfunc='mean'\n",
    ").cumsum(axis=1).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fc108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roi_piv.index=roi_piv.index.astype(str)\n",
    "sns.heatmap(roi_piv, annot=True, fmt='.2f', linewidths=1, linecolor='grey', cbar_kws= {'orientation': 'horizontal'} \n",
    "            ).set(title ='ROI per cohort')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74211a26",
   "metadata": {},
   "source": [
    "Again we see the best results for September cohort reaching ROI 1.42  and June cohort ROI 1.33. Only these cohorts'  ROI is above 1, so campaign's gross profit exceeds expenses. Other cohorts have ROI smaller than 1, so for them ad campaing doesn't pay off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bc4f7-dd69-4dbb-939d-fcee3300a772",
   "metadata": {},
   "source": [
    "#### ROI per source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1063e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ltv_source=orders.groupby(['first_source'])[['uid','revenue']].agg({'uid':'nunique','revenue':'sum'}).reset_index()\n",
    "ltv_source.columns=['source_id','customers','revenue']\n",
    "ltv_source['ltv']=ltv_source['revenue']/ltv_source['customers']\n",
    "ltv_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1792a-7a20-46de-bce7-0186805db7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_source=marketing_source.reset_index()\n",
    "marketing_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaec3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_source=marketing_source.merge(ltv_source,on=['source_id'])\n",
    "roi_source['cac']=(roi_source['costs']/roi_source['customers']). round(2)\n",
    "roi_source['romi']=(roi_source['ltv']/roi_source['cac']).round(2)\n",
    "roi_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747896cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.bar(roi_source, x='source_id', y='romi')\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9673c3",
   "metadata": {},
   "source": [
    "So, source 1 has max ROMI(1.8), sources 2 and 5 also have ROMI > 1 ,so they are profitable. Source 9 has ROMY equal to 1. Source 3 has min ROMI 0.37, sources 10 - 0.78, source 4 - 0.9. Probably sources 3 and 10 should be cut out, while the flow from sources 1,2,5 should be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a779bef-edb7-49ab-90ee-764aca1d5676",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Marketing metrics were calculated and analized.\n",
    "\n",
    "**total costs** spent on marketing is $329131.62. Max **share per source** - 43% of all costs - is from source 3, min costs -  less than 2% - from sources 9,10.\n",
    "**Over time**: the highest peak for almost all sources is on Nov 24 - as expected, min costs were spent on Mar 31(end of quarter?) and in august(people on holidays). There're also some minor peaks for sources 1, 4, 5 which often(not always) match. For some reason there's no special activity from source 4 around Nov 24 -  on the contrary, its costs are pretty small on Nov 24.\n",
    "The distribution of total costs over time is compered with:\n",
    "-  **DAU/WAU/MAU/number of sessions per day** lineplots -  very similar to  total costs, with the same peak on Nov 24  and min costs spent in August and sudden fall on Mar 31. \n",
    "- **revenue** distribution over time - less correlation found. The highest revenue peak is not on Nov 24, but on Dec 10 (related with max activity of source 5 supported by other sources? people buy more expensive goods befor the main holidays) and May 31. Revenue in January falls below average(users have spent all the money on expensive presents) till Jan 26, Feb 1 and stay above average till the end of March. Min revenue on Mar 31 and in august correlates with min marketing costs.\n",
    "\n",
    "**Marketing costs per month**: max costs are spent on marketing in December, November, October, the smallest costs are is August. In Oct, Nov and Dec marketing costs gradually grow, that correlates with growing revenues and max revenue in December. In January costs remain relatively high, though revenue falls below average. Costs decrease from jan to March, while DAU/MAU doesn't change much and revenue per day grows towards March. Costs in june and July are the same, but revenue and users activity rises significantly around July 17. Mayby marketing costs should support more natural user's activity in February, March and July?\n",
    "\n",
    "**Marketing costs by day of the week**: regular weakly cycles are seen on the graph for daily costs like on DAU and revenue graphs. Max marketing costs are spent on Wednesday, then Thursday and Sunday, min costs are on Saturday. As we have already seen users open the app most often on Wednesday and Sunday, while min number of visits is on Saturday and Friday. Maybe the main target days should be  Wednesday and Sunday(not Thursday) to attract more customers.\n",
    "\n",
    "**CAC over time**: Max CAC is in August(10,8), when user's activity is the least, then it goes down till October(8.4), when users become more active. In November attracting a customer costs more than in December(users are active without special push) and March, but less than in January and April(users don't want to buy). Min CAC is in May(7,4).\n",
    "\n",
    "**CAC per source**:\n",
    "Sources 3 and 2 spend much more on attracting a customer than other sources and they bring many customers. But these sources are not so effective as sources 5,4,1: Source 4 brings almost as many customers as source 3, but each customers price is 2 times lower, source 1 brings the same number of customers as source 2 at 2 times lower cost.\n",
    "Maybe there's a possibility to reduce costs on less effective sources in favor of most effective sources.\n",
    "\n",
    "**ROI** in cohort analysis\n",
    "The best results for September cohort reaching ROI 1.42  and June cohort ROI 1.33. Only these cohorts'  ROI is above 1, so campaign's gross profit exceeds expenses. Other cohorts have ROI smaller than 1, so for them ad campaing doesn't pay off. \n",
    "\n",
    "**ROI per source**\n",
    "source 1 has max ROMI(1.8), sources 2 and 5 also have ROMI > 1 ,so they are profitable. Source 9 has ROMY equal to 1. Source 3 has min ROMI 0.37, sources 10 - 0.78, source 4 - 0.9. Probably sources 3 and 10 should be cut out, while the flow from sources 1,2,5 should be increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d36109",
   "metadata": {},
   "source": [
    "# Step 3. Write a conclusion: advise marketing experts how much money to invest and where.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea9c1d-cfa9-4cc8-9333-74122bd3cfde",
   "metadata": {},
   "source": [
    "Our conclusions about user's activity, sales and marketing metrics help to find problems with the app and recommend some changes in marketing organization.\n",
    "\n",
    "**Cohorts** max activity was shown by June and September cohorts - max retention, max cumulatibe orders per user, revenue, LTV , ROI, while other cohorts were only effective in some periods by some metrics( eg.december cohort had max revenue in February, March, April). Theese profitable cohorts are supported not only by natural factors such as rises in users activity over the year, but also by special marketing efforts. Marketing department should use this positive experience to support other cohorts. \n",
    "\n",
    "**Sources** Max share of marketing costs(43% of all costs) is related with source 3, min costs (less than 2%) with  sources 9,10.  Sources 4 ,5 , 1 get only 18.6%, 15.7% , 6,3% respectively (summed less than source 3), but they are much more effective! Comparing CAC per source shows that sources 3 and 2 have max CAC(14.2) 12.3), so source 4 brings almost as many customers as source 3 with  2 times lower CAC(6.6), source 5 brings 2 times more customers than source 2 with 2 times lower CAC? source 1 brings the same number of customers as source 2 at 2 times lower cost. Analizing return on marketing investments we find that only sources 1,2,5 have ROMI >1 ( max 1.8 - source 1) and make profit, source 9 has ROMY equal to 1, other sources don't pay off. Source 3 has min ROMI 0.37, sources 10 - 0.78. Probably sources 3 and 10 should be cut out, while the flow from sources 1,2,5 should be increased to reduce costs on less effective sources in favor of most effective sources.\n",
    "\n",
    "**Over time** \n",
    "In general marketing costs are distributed over year in correlation with user's activity, which is above average since October till April and has peaks on  Nov 24, May 31 and is minimum in August when people are on vacation.  \n",
    "- The majority of sources spend max costs on Nov 24, which is Black Friday, and users also demonstrate max activity on that day. But the highest revenue peak is not on Nov 24, but on Dec 10, which correlates with max costs spend on source 5(maybe people by more tickets before winter hilidays). We may suppose that if other sources concentrate not only on Nov 24, but also on December sales, they will get more revenue. \n",
    "- In January revenue falls below average (users have already spent all their money), and rises again by Jan 26 - Feb 1, then stays above average till the end of March. But in January marketing costs remain relatively high, then costs decrease from January to March(while DAU/MAU doesn't change much and revenue per day grows towards March). Marketing costs in June and July are equal, but revenue and users activity rises significantly around July 17. We suppose that costs destribution should take into account not only user's activity, but also revenue distribution over the year, thus costs would boost natural user's activity and bigger revenue in February, March and July.\n",
    "- Marketing costs per month: max costs are spent on marketing in December, November, October, the smallest costs are is August. In Oct, Nov and Dec marketing costs gradually grow, that correlates with growing revenues and max revenue in December. From analizing CAC we know that max CAC is in August(10,8), when user's activity is the least, then it goes down till October(8.4), when users become more active. In November attracting a customer costs more than in December(users are active without special push) and March, but less than in January and April(users don't want to buy). Min CAC is in May(7,4). This picture gives one more reason to increase marceting costs in December, March and May and decrease in August.\n",
    "- Day of week: Maximum user activity occurs on Wednesday and Sunday( max DAU, number of sessions per user, session length), but now max marketing costs are spent on Wednnesday and Thursday. It would be better to focus on Sunday instead of Thursday, so the main target days would be Wednesday and Sunday(not Thursday) to attract more customers.\n",
    "\n",
    "\n",
    "**Technical issues**\n",
    "- 10% of sessions last less than 15 sec -  that can be a signal of some issues with the app (starting page loading, registration form etc) Tecnical support should identify and fix these issues.\n",
    "- There was a drop down in all product metrics as well as sales and marketing metrics on March 31. Check if there were any problems with the app which could explain the issue.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3bfc5-6ec6-4ae0-ae67-a9c41ae18d63",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
